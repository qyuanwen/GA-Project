{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2a127c",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "#  Capstone Project: Train Delays Predictor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651dd29f",
   "metadata": {},
   "source": [
    "### Problem Statement and Solution Approach:\n",
    "\n",
    "**Problem:**<br>\n",
    "Despite ongoing efforts to improve the MRT system, train delays and faults persist, causing frustration and inconvenience for passengers.\n",
    "\n",
    "**Proposed Solution:**<br>As a daily commuter of Singapore MRT, I aim to develop a train delays predictor that can identify stations and timings that are more likely to experience breakdowns or delays. \n",
    "\n",
    "By analyzing historical data on time of day, type of day, station name, commuter volume, and breakdown/non-breakdown indicators, I hope to create a model that can accurately predict future breakdowns and help commuters avoid stations and times with potential delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e4210",
   "metadata": {},
   "source": [
    "## 1. Data Collection: Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e560e92",
   "metadata": {},
   "source": [
    "### Context:\n",
    "Scraping data from SMRT twitter profile tweets with two columns being Timestamp and Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a623ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "import selenium\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370940e6",
   "metadata": {},
   "source": [
    "**Set up the scraper using selenium**\n",
    "\n",
    "**Set up some conditions (for example, I only want to scrape data from the period 2018 to 2023)**\n",
    "\n",
    "**Start scraping!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b692a25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of timestamps collected: 735\n",
      "Number of tweets collected: 8359\n"
     ]
    }
   ],
   "source": [
    "# Set up the webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the Twitter page you want to scrape\n",
    "url = \"https://twitter.com/SMRT_Singapore\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "sleep(5)\n",
    "\n",
    "# Set the start and end years you want to scrape\n",
    "start_year = 2017\n",
    "end_year = 2023\n",
    "\n",
    "# Find the articles on the page\n",
    "articles = driver.find_elements(By.XPATH,\"//article[@data-testid='tweet']\")\n",
    "\n",
    "# Create an empty dictionary to store the timestamps and tweets\n",
    "tweets_dict = {}\n",
    "\n",
    "# Loop through the articles on the page\n",
    "while True:\n",
    "    for article in articles:\n",
    "        # Get the timestamp and tweet text\n",
    "        timestamp = article.find_element(By.XPATH,\".//time\").get_attribute('datetime')\n",
    "        tweet = article.find_element(By.XPATH,\".//div[@data-testid='tweetText']\")\n",
    "\n",
    "        # Parse the year from the timestamp\n",
    "        year = int(timestamp[:4])\n",
    "        \n",
    "        # If the year is within the desired range, add the timestamp and tweet to the dictionary\n",
    "        if year >= start_year and year <= end_year:\n",
    "            if timestamp not in tweets_dict:\n",
    "                tweets_dict[timestamp] = []\n",
    "            tweets_dict[timestamp].append(tweet.text)\n",
    "    \n",
    "    # Scroll to the bottom of the page\n",
    "    # driver.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "\n",
    "    # Scroll half a page\n",
    "    driver.execute_script(\"window.scrollBy(0, window.innerHeight/2);\")\n",
    "\n",
    "\n",
    "    # Wait for the page to load new data for 3 to 5 seconds\n",
    "    sleep(random.uniform(3, 5))\n",
    "    \n",
    "    # Get the new list of articles\n",
    "    new_articles = driver.find_elements(By.XPATH,\"//article[@data-testid='tweet']\")\n",
    "    \n",
    "    # If there are no new articles, assume we have reached the end of the page and exit the loop\n",
    "    if not new_articles:\n",
    "        break\n",
    "    \n",
    "    # Update the articles list\n",
    "    articles = new_articles\n",
    "    \n",
    "    # Check if the last tweet collected was from the latest year\n",
    "    last_year = int(list(tweets_dict.keys())[-1][:4])\n",
    "    if last_year <= start_year:\n",
    "        break\n",
    "\n",
    "# Print the number of timestamps and tweets collected\n",
    "print(f\"Number of timestamps collected: {len(tweets_dict)}\")\n",
    "print(f\"Number of tweets collected: {sum([len(v) for v in tweets_dict.values()])}\")\n",
    "\n",
    "# Quit the webdriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015e42d",
   "metadata": {},
   "source": [
    "**After scraping, transform scraped data which was stored in dictionary into dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aef19d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Timestamp                                              Tweet\n",
      "0  2023-03-14T09:30:00.000Z  [Have you shared your views yet?#ICYMI: We're ...\n",
      "1  2023-03-09T01:43:20.000Z  [[BPLRT] Train services are running normally.,...\n",
      "2  2023-03-08T13:00:03.000Z  [[BPLRT] UPDATE: Additional 10mins train trave...\n",
      "3  2023-03-08T12:51:47.000Z  [[BPLRT] UPDATE: Additional 15mins train trave...\n",
      "4  2023-03-08T12:42:24.000Z  [[BPLRT] CLEARED: Train services has resumed. ...\n"
     ]
    }
   ],
   "source": [
    "# create a list of tuples from the tweets_dict dictionary\n",
    "tweets_list = [(k, v) for k, v in tweets_dict.items()]\n",
    "\n",
    "# create a DataFrame from the list of tuples\n",
    "df = pd.DataFrame(tweets_list, columns=['Timestamp', 'Tweet'])\n",
    "\n",
    "# print the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70522a21",
   "metadata": {},
   "source": [
    "**Export scraped data to Excel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99819465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22004\\2318764703.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/qiyua/Desktop/QIYUAN/CAREER/TFIP/TFIP_General Assembly/Course materials/CAPSTONE PROJECT/excel/tweets_live8.xlsx\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_excel(\"C:/Users/qiyua/Desktop/QIYUAN/CAREER/TFIP/TFIP_General Assembly/Course materials/CAPSTONE PROJECT/excel/tweets_live8.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767994be",
   "metadata": {},
   "source": [
    "**We will continue the rest of the analysis in a separate workbook. Please refer to \"2. Data Cleaning and Data Analysis\" for the analysis and recommendations.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
